{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Which listing toÂ select?\n",
    "### 2. What ratings and reviews tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet') \n",
    "import textblob\n",
    "import langdetect\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langdetect import detect\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator \n",
    "\n",
    "\n",
    "# Pre: read review data and remove non-English comments\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "\n",
    "reviews['comments'] = reviews['comments'].astype(str)\n",
    "reviews['language'] = None\n",
    "for i in range(len(reviews)):\n",
    "    try:\n",
    "        reviews['language'][i] = detect(reviews['comments'][i])\n",
    "    except:\n",
    "        reviews['language'][i] = \"non-en\"   #language detection from nltk is sensitive to empty string\n",
    "reviews=reviews.query('language ==\"en\" ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.Scatter plot showing relationship between rating scores and number of reviews\n",
    "lists.plot(kind='scatter', y='reviews_per_month', x='review_scores_rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.Wordcloud of reviews for each listing\n",
    "file = open('stopwords.txt','r')\n",
    "stopwords = []\n",
    "for line in file:\n",
    "    line = line.rstrip().strip(' ')\n",
    "    stopwords.append(line)  \n",
    "    \n",
    "def comment(id):\n",
    "    '''\n",
    "    Generate a dataframe with all the comments of the listing with the id you input\n",
    "    '''\n",
    "    dic = {}\n",
    "    dic2 = {}\n",
    "    for index, row in reviews.iterrows():\n",
    "        if row['listing_id'] == id :\n",
    "            dic[index] = row['comments']\n",
    "            dic2[index] = row['listing_id']\n",
    "    df = pd.DataFrame({'listing_id':dic2, 'comments':dic})  \n",
    "    return df\n",
    "    \n",
    "def wordcloud(id):\n",
    "    '''\n",
    "    Generate wordcloud for the listing with the id you input\n",
    "    '''  \n",
    "    df = comment(id)\n",
    "    # text cleaning for listing comments\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join(x.lower() for x in x.split()))   #lower cases\n",
    "    df['comments'] = df['comments'].str.replace('[^\\w\\s]','')   #remove punctuation\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))  #remove stop words\n",
    "    df['comments'] = df['comments'].apply(lambda x: str(TextBlob(x).correct()))  #correct spelling \n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))  #lemmatization\n",
    "    \n",
    "    # create wordcloud\n",
    "    total_comments = \" \".join(com for com in df.comments) \n",
    "    wordcloud = WordCloud(background_color = \"white\").generate(total_comments)\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"wordcloud.png\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.Clustering Summarization\n",
    "def  summarization(id):\n",
    "    '''\n",
    "    Generate summarization for each listing with the id you input\n",
    "    '''\n",
    "    df = comment(id)\n",
    "    # tokenize English sentences\n",
    "    eng_comments = \" \".join(sent for sent in comment['comments'])\n",
    "    tok_comments = sent_tokenize(eng_comments)\n",
    "    \n",
    "    # skip thoughts sentence embedding\n",
    "    all_com = [sent for sent in tok_comments]\n",
    "    enc_com = encoder.encode(all_sent, verbose=False)   #pre-trained model can be checked in #Appendix#\n",
    "    \n",
    "    # KMeans clustering\n",
    "    n_clusters = int(np.ceil(len(enc_com)**0.2))  #determine number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans = kmeans.fit(enc_com)\n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, enc_com)\n",
    "    summary = \" \".join(cleaned_text[idx] for idx in closest)\n",
    "    print(summary)\n",
    "\n",
    "    \n",
    "## Appendix: Import pre-trained Skip Thoughts encode model\n",
    "import scipy.spatial.distance as sd\n",
    "import configuration\n",
    "import encoder_manager\n",
    "\n",
    "VOCAB_FILE = \"../skip_thoughts_bi_2017_02_16/vocab.txt\"\n",
    "EMBEDDING_MATRIX_FILE = \"../skip_thoughts_bi_2017_02_16/embeddings.npy\"\n",
    "CHECKPOINT_PATH = \"../skip_thoughts_bi_2017_02_16/model.ckpt-500008\"\n",
    "\n",
    "encoder = encoder_manager.EncoderManager()\n",
    "encoder.load_model(configuration.model_config(bidirectional_encoder=True),\n",
    "                   vocabulary_file=VOCAB_FILE,\n",
    "                   embedding_matrix_file=EMBEDDING_MATRIX_FILE,\n",
    "                   checkpoint_path=CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.Sentiment Analysis\n",
    "reviews['sentiment'] = reviews['comments'].apply(lambda x: TextBlob(x).sentiment[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
