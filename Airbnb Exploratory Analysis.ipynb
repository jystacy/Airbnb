{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import descartes\n",
    "\n",
    "\n",
    "## Read data\n",
    "lists = pd.read_csv('listings.csv')\n",
    "pd.set_option('display.max_columns', 500) #expand the size of terminal window to display all rows\n",
    "\n",
    "## Clean data - only for this case, using LA as example\n",
    "lists['zipcode'] = lists['zipcode'].astype(str).\\\n",
    "                                    str.replace('ca','').\\\n",
    "                                    str.replace('CA','').\\\n",
    "                                    str.replace('Near ','').\\\n",
    "                                    str.replace('139 S Valencia Ave, Glendora.','').\\\n",
    "                                    str.extract(r'^(\\d{5})')\n",
    "\n",
    "lists['price'] = lists['price'].astype(str).\\\n",
    "                                str.replace('$','').\\\n",
    "                                str.replace(',','').\\\n",
    "                                astype(float)\n",
    "\n",
    "lists = lists.query('price > 0')  #filter out listings for free living\n",
    "lists['cleaning_fee'] = lists['cleaning_fee'].astype(str).\\\n",
    "                                            str.replace('$','').\\\n",
    "                                            str.replace(',','').\\\n",
    "                                            astype(float)\n",
    "\n",
    "lists['host_since_year'] = lists['host_since'].astype(str).apply(lambda x: x.split('-')[0]) #extract host join year\n",
    "\n",
    "lists['list_since_year'] = lists['first_review'].astype(str).apply(lambda x: x.split('-')[0]) #extract listing upload year\n",
    "\n",
    "## Summarize data\n",
    "#lists.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Description of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bar chart showing the new listing growth rate from 2008\n",
    "list_growth = pd.DataFrame()\n",
    "list_growth = lists.groupby('list_since_year')['id'].nunique().reset_index()\n",
    "list_growth = list_growth.query('list_since_year != \"nan\"').rename(columns = {'id': 'new_listing_count'})\n",
    "list_growth.plot(kind = 'bar', x = 'list_since_year', title = 'New Listing Growth Trend')\n",
    "\n",
    "## Bar chart showing the new host growth rate from 2008\n",
    "host_growth = pd.DataFrame()\n",
    "host_growth = lists.groupby('host_since_year')['host_id'].nunique().reset_index()\n",
    "host_growth = host_growth.query('host_since_year != \"nan\"').rename(columns = {'host_id': 'new_host_count'})\n",
    "host_growth.plot(kind = 'bar', x = 'host_since_year', title = 'New Host Growth Trend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Where to live?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps  #heatmap\n",
    "import os\n",
    "import requests\n",
    "from requests import RequestException  #automatic extraction\n",
    "\n",
    "## Preparation\n",
    "listzipcode = pd.DataFrame()\n",
    "listzipcode['count'] = lists.groupby(['zipcode']).count()['id']\n",
    "listzipcode['avgprice'] = lists.groupby(['zipcode']).mean()['price']\n",
    "listzipcode['avgscore'] = lists.groupby(['zipcode']).mean()['review_scores_rating']\n",
    "listzipcode['location'] = lists.groupby(['zipcode']).mean()['review_scores_location']\n",
    "listzipcode['value'] = lists.groupby(['zipcode']).mean()['review_scores_value']\n",
    "listzipcode = listzipcode.reset_index()\n",
    "\n",
    "## 1.Heat map of Airbnb listing density in Los Angeles\n",
    "# link to get google map api_key: https://developers.google.com/maps/documentation/embed/get-api-key\n",
    "gmaps.configure(api_key = \"AIzaSyATnwHhxPqq3QN7q5DSIfcMQOTjsStXF30\")   \n",
    "location_columns = lists[['latitude', 'longitude']]\n",
    "location_tuples = [tuple(x) for x in location_columns.values]\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(gmaps.heatmap_layer(location_tuples))\n",
    "fig\n",
    "\n",
    "## 2.Choropleth of Crime Rate by zip code in Los Angeles\n",
    "# Read shapefile of ZIP Code Tabulation Areas (ZCTAs) geometry of Great Los Angeles Area  \n",
    "shapefile = gpd.read_file(\"tl_2018_us_zcta510.shp\")[['ZCTA5CE10','geometry']]\n",
    "shapefile.rename(columns = {'ZCTA5CE10': 'zipcode'}, inplace = True)\n",
    "la_zip = pd.read_excel('lazip.xlsx').astype(str)\n",
    "shapefile = shapefile.merge(la_zip, on = 'zipcode', how = 'inner')\n",
    "\n",
    "# Scrape crime rate of zip code level from www.bestplaces.net and merge to listzipcode.csv\n",
    "procrime = {}\n",
    "for i in listzipcode['zipcode']:\n",
    "    url = 'https://www.bestplaces.net/crime/zip-code/*/*'\n",
    "    url = os.path.join(url, i)\n",
    "    html = requests.get(url).text\n",
    "    if \"property crime is\" in html:\n",
    "        procrime[i] = html.split(\"violent crime is\")[1].split()[0].replace('.<small>','')\n",
    "    else:\n",
    "        procrime[i] = 'NaN'\n",
    "crime_rate = pd.DataFrame({'crimerate':procrime}).reset_index()\n",
    "crime_rate.rename(columns = {'index': 'zipcode'}, inplace = True)\n",
    "listzipcode = listzipcode.merge(crime_rate, on = 'zipcode', how = 'left')\n",
    "\n",
    "# Merge geodata with cleaned crime rate dataset\n",
    "listchoropleth = shapefile.merge(listzipcode, on='zipcode', how='right')\n",
    "\n",
    "# Create choropleth of Crime Rate by zip code in Los Angeles\n",
    "variable_crime = 'crimerate'\n",
    "vmin, vmax = 50, 100\n",
    "fig, ax = plt.subplots(1, figsize = (20, 10))\n",
    "ax.set_xlim([-119.1, -117.4])\n",
    "ax.set_ylim([33.6, 34.9])\n",
    "ax.axis('off')\n",
    "ax.set_title('Violent Crime by Zip Code in Los Angeles', \\\n",
    "             fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "ax.annotate('Source: www.bestplaces.net', xy = (0.1, .08), \\\n",
    "            xycoords = 'figure fraction', horizontalalignment = 'left', \\\n",
    "            verticalalignment = 'top', fontsize = 12, color = '#555555')  \n",
    "\n",
    "img_crime = listchoropleth.plot(column = variable_crime, scheme = 'fisher_jenks', \\\n",
    "                                cmap = 'Reds', linewidth = 0.8, ax = ax, edgecolor = '0.8')\n",
    "# Create colorbar as legend\n",
    "sm_crime = plt.cm.ScalarMappable(cmap = 'Reds', norm = plt.Normalize(vmin = vmin, vmax = vmax)) \n",
    "sm_crime._A = []\n",
    "cbar_crime = fig.colorbar(sm_crime)\n",
    "\n",
    "## 3.Choropleth of Average Price by zip code in Los Angeles\n",
    "variable_price = 'avgprice'\n",
    "vmin, vmax = 0, 2000\n",
    "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
    "ax.set_xlim([-119.1, -117.4])\n",
    "ax.set_ylim([33.6, 34.9])\n",
    "ax.axis('off')\n",
    "ax.set_title('Average Price by Zip Code in Los Angeles', \\\n",
    "             fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "ax.annotate('Source: Inside Airbnb', xy = (0.1, .08), \\\n",
    "            xycoords = 'figure fraction', horizontalalignment = 'left', \\\n",
    "            verticalalignment = 'top', fontsize = 12, color = '#555555')\n",
    "\n",
    "img_price = listchoropleth.plot(column = variable_price, scheme='fisher_jenks', \\\n",
    "                                cmap = 'Blues', linewidth = 0.8, ax = ax, edgecolor = '0.8')\n",
    "\n",
    "sm_price = plt.cm.ScalarMappable(cmap = 'Blues', norm = plt.Normalize(vmin = vmin, vmax = vmax))\n",
    "sm_price._A = []\n",
    "cbar_price = fig.colorbar(sm_price)\n",
    "\n",
    "\n",
    "## 4.Line chart showing rating distribution of most expensive listings\n",
    "# listzipcode.query('avgprice > 1000')\n",
    "df_price = lists[['zipcode',\n",
    "                'review_scores_accuracy',\n",
    "                'review_scores_cleanliness',\n",
    "                'review_scores_checkin',\n",
    "                'review_scores_communication',\n",
    "                'review_scores_location',\n",
    "                'review_scores_value']].query ('zipcode in [\"90077\",\"90210\",\"90265\"]')\n",
    "df_price = df_price.rename(columns = {'review_scores_accuracy': 'accuracy', \n",
    "                                    'review_scores_cleanliness': 'cleanliness', \n",
    "                                    'review_scores_checkin': 'checkin',\n",
    "                                    'review_scores_communication':'communication',\n",
    "                                    'review_scores_location':'location',\n",
    "                                    'review_scores_value': 'value'})\n",
    "high_price = df_price[['accuracy',\n",
    "               'cleanliness',\n",
    "               'checkin',\n",
    "               'communication',\n",
    "               'location',\n",
    "               'value']].mean()\n",
    "high_price.plot(kind = 'line', title = 'Rating Distribution of Most Expensive Listings' )\n",
    "\n",
    "## 5.Horizontal bar chart for 20 neighbourhoods with highest score of value\n",
    "listvalue = pd.DataFrame()\n",
    "listvalue['count'] = lists.groupby(['neighbourhood_cleansed']).count()['id']\n",
    "listvalue['value'] = lists.groupby(['neighbourhood_cleansed']).mean()['review_scores_value']\n",
    "listvalue = listvalue.reset_index().\\\n",
    "                        query('count>100').\\\n",
    "                        sort_values(by = 'value').\\\n",
    "                        tail(20).\\\n",
    "                        rename(columns = {'neighbourhood_cleansed':'neighbourhood'})\n",
    "\n",
    "listvalue.plot(kind = 'barh', \n",
    "               figsize = (8, 8), \n",
    "               y = 'value', \n",
    "               x = 'neighbourhood', \n",
    "               title = '20 Neighbourhoods with Highest Score of Value', \n",
    "               xlim = (9.4,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Which listing to select?\n",
    "### 1. What kind of hosts can be trusted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.Airbnb authentication policies - \"Superhost\", identity verification, host profile pic\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns \n",
    "from statsmodels.stats.multicomp import (pairwise_tukeyhsd, MultiComparison)\n",
    "\n",
    "listhost = lists.groupby(['host_id'])['host_total_listings_count',\n",
    "                                    'host_response_rate',\n",
    "                                    'number_of_reviews',\n",
    "                                    'review_scores_rating',\n",
    "                                    'reviews_per_month'].mean()\n",
    "df = lists[['host_id',\n",
    "             'host_is_superhost',\n",
    "             'host_has_profile_pic',\n",
    "             'host_response_time',\n",
    "             'host_identity_verified',\n",
    "             'host_verifications',\n",
    "             'host_neighbourhood']]\n",
    "listhost = listhost.merge(df, on = 'host_id', how = 'left')\n",
    "\n",
    "sns.boxplot(x = \"host_is_superhost\", y = \"review_scores_rating\", data = listhost, showfliers = False)\n",
    "ttest_ind(listhost.query('host_is_superhost == \"t\"').dropna()['review_scores_rating'],\n",
    "          listhost.query('host_is_superhost == \"f\"').dropna()['review_scores_rating'])\n",
    "\n",
    "sns.boxplot(x = \"host_has_profile_pic\", y = \"review_scores_rating\", data = listhost, showfliers = False)\n",
    "ttest_ind(listhost.query('host_has_profile_pic == \"t\"').dropna()['review_scores_rating'],\n",
    "          listhost.query('host_has_profile_pic == \"f\"').dropna()['review_scores_rating'])\n",
    "\n",
    "sns.boxplot(x = \"host_identity_verified\", y = \"review_scores_rating\", data = listhost, showfliers = False)\n",
    "ttest_ind(listhost.query('host_identity_verified == \"t\"').dropna()['review_scores_rating'],\n",
    "          listhost.query('host_identity_verified == \"f\"').dropna()['review_scores_rating'])\n",
    "\n",
    "## 2.Host's cancellation policy\n",
    "\n",
    "# lists.query('cancellation_policy == \"super_strict_30\"') #only 9 listings with this policy\n",
    "# lists.query('cancellation_policy == \"strict\"') #only 63 listings with this policy which has changed to \"strict_14_with_grace_period\" from 2018\n",
    "# remove \"super_strict_30\" and \"strict\"\n",
    "\n",
    "list_cancel = lists[['review_scores_rating', 'cancellation_policy']].dropna()\n",
    "list_cancel = list_cancel.query('cancellation_policy !=[\"super_strict_30\",\"strict\"]')\n",
    "res = pairwise_tukeyhsd(list_cancel['review_scores_rating'], list_cancel['cancellation_policy'])\n",
    "res.plot_simultaneous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What ratings and reviews tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet') \n",
    "import textblob\n",
    "import langdetect\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langdetect import detect\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator \n",
    "\n",
    "\n",
    "# Pre: read review data and remove non-English comments\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "\n",
    "reviews['comments'] = reviews['comments'].astype(str)\n",
    "reviews['language'] = None\n",
    "for i in range(len(reviews)):\n",
    "    try:\n",
    "        reviews['language'][i] = detect(reviews['comments'][i])\n",
    "    except:\n",
    "        reviews['language'][i] = \"non-en\"   #language detection from nltk is sensitive to empty string\n",
    "reviews=reviews.query('language ==\"en\" ')\n",
    "\n",
    "\n",
    "## 1.Scatter plot showing relationship between rating scores and number of reviews\n",
    "lists.plot(kind='scatter', y='reviews_per_month', x='review_scores_rating')\n",
    "\n",
    "\n",
    "## 2.Wordcloud of reviews for each listing\n",
    "file = open('stopwords.txt','r')\n",
    "stopwords = []\n",
    "for line in file:\n",
    "    line = line.rstrip().strip(' ')\n",
    "    stopwords.append(line)  \n",
    "    \n",
    "def comment(id):\n",
    "    '''\n",
    "    Generate a dataframe with all the comments of the listing with the id you input\n",
    "    '''\n",
    "    dic = {}\n",
    "    dic2 = {}\n",
    "    for index, row in reviews.iterrows():\n",
    "        if row['listing_id'] == id :\n",
    "            dic[index] = row['comments']\n",
    "            dic2[index] = row['listing_id']\n",
    "    df = pd.DataFrame({'listing_id':dic2, 'comments':dic})  \n",
    "    return df\n",
    "    \n",
    "def wordcloud(id):\n",
    "    '''\n",
    "    Generate wordcloud for the listing with the id you input\n",
    "    '''  \n",
    "    df = comment(id)\n",
    "    # text cleaning for listing comments\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join(x.lower() for x in x.split()))   #lower cases\n",
    "    df['comments'] = df['comments'].str.replace('[^\\w\\s]','')   #remove punctuation\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))  #remove stop words\n",
    "    df['comments'] = df['comments'].apply(lambda x: str(TextBlob(x).correct()))  #correct spelling \n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))  #lemmatization\n",
    "    \n",
    "    # create wordcloud\n",
    "    total_comments = \" \".join(com for com in df.comments) \n",
    "    wordcloud = WordCloud(background_color = \"white\").generate(total_comments)\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"wordcloud.png\") \n",
    "\n",
    "\n",
    "## 3.Clustering Summarization\n",
    "def  summarization(id):\n",
    "    '''\n",
    "    Generate summarization for each listing with the id you input\n",
    "    '''\n",
    "    df = comment(id)\n",
    "    # tokenize English sentences\n",
    "    eng_comments = \" \".join(sent for sent in comment['comments'])\n",
    "    tok_comments = sent_tokenize(eng_comments)\n",
    "    \n",
    "    # skip thoughts sentence embedding\n",
    "    all_com = [sent for sent in tok_comments]\n",
    "    enc_com = encoder.encode(all_sent, verbose=False)   #pre-trained model can be checked in #Appendix#\n",
    "    \n",
    "    # KMeans clustering\n",
    "    n_clusters = int(np.ceil(len(enc_com)**0.2))  #determine number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans = kmeans.fit(enc_com)\n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, enc_com)\n",
    "    summary = \" \".join(cleaned_text[idx] for idx in closest)\n",
    "    print(summary)\n",
    "\n",
    "    \n",
    "## 4.Sentiment Analysis\n",
    "reviews['sentiment'] = reviews['comments'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "\n",
    "\n",
    "## Appendix: Import pre-trained Skip Thoughts encode model\n",
    "import scipy.spatial.distance as sd\n",
    "import configuration\n",
    "import encoder_manager\n",
    "\n",
    "VOCAB_FILE = \"../skip_thoughts_bi_2017_02_16/vocab.txt\"\n",
    "EMBEDDING_MATRIX_FILE = \"../skip_thoughts_bi_2017_02_16/embeddings.npy\"\n",
    "CHECKPOINT_PATH = \"../skip_thoughts_bi_2017_02_16/model.ckpt-500008\"\n",
    "\n",
    "encoder = encoder_manager.EncoderManager()\n",
    "encoder.load_model(configuration.model_config(bidirectional_encoder=True),\n",
    "                   vocabulary_file=VOCAB_FILE,\n",
    "                   embedding_matrix_file=EMBEDDING_MATRIX_FILE,\n",
    "                   checkpoint_path=CHECKPOINT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
